{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6ccc1472ecdf42d69e0917e2f631f76e",
            "b136b9003bef44838e369753e1a50252",
            "d2527b3edcf2410b88af18d2529062a4",
            "7540edfe79074ab68decbadd125bb7b4",
            "15152bbcd18d40989aac264aca6d5b1c",
            "2b5a6b7e557c445e8ce85b0888caa68e",
            "7d876db4f40148f299df41e888691e14",
            "f80326cb8b3e4459a69d01ade309db93",
            "1ff2c7f3aeae42a0b285826c7c65569f",
            "ea5ce38ca2884ff1beee617a37f37b9e",
            "5030d2373dfd492aaedf703d8ecac028"
          ]
        },
        "id": "p4fmBuIsFTir",
        "outputId": "6973c693-c8ea-423b-83ea-c35af3f59c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-2.24.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.17.1+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2023.12.25)\n",
            "Collecting ftfy (from open-clip-torch)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (4.66.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.20.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (3.20.3)\n",
            "Collecting timm (from open-clip-torch)\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.0->open-clip-torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (24.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->open-clip-torch) (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open-clip-torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm, open-clip-torch\n",
            "Successfully installed ftfy-6.2.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 open-clip-torch-2.24.0 timm-0.9.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ccc1472ecdf42d69e0917e2f631f76e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install open-clip-torch\n",
        "\n",
        "import open_clip\n",
        "import tqdm\n",
        "import torch\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model, _ , preprocess = open_clip.create_model_and_transforms(\n",
        "    \"ViT-B-32\",\n",
        "    pretrained=\"laion2b_s34b_b79k\"\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BKIRl59cFali"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def embeddings_to_class_probs(vision_embeddings, text_embeddings):\n",
        "    vision_embeddings = vision_embeddings / vision_embeddings.norm(dim=-1, keepdim=True)\n",
        "    text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
        "    logits = vision_embeddings @ text_embeddings.T\n",
        "    class_probs = F.softmax(100. * logits, dim=-1)\n",
        "    return class_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4doj-QcLzNZ"
      },
      "source": [
        "### SLT10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6cE8oIBVbrL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "labels = [\n",
        "    \"an airplane\",\n",
        "    \"a bird\",\n",
        "    \"a car\",\n",
        "    \"a cat\",\n",
        "    \"a deer\",\n",
        "    \"a dog\",\n",
        "    \"a horse\",\n",
        "    \"a monkey\",\n",
        "    \"a ship\",\n",
        "    \"a truck\"\n",
        "]\n",
        "\n",
        "text = tokenizer(labels).to(device)\n",
        "text_embeddings = model.encode_text(text).to(device)\n",
        "\n",
        "linear_probe = nn.Linear(512, len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvVAlRpaL35h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7586dbb5-33b0-47bf-bd87-aaab4cd7cf11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import STL10\n",
        "\n",
        "optimizer = torch.optim.Adam(linear_probe.parameters(), lr=3e-4)\n",
        "\n",
        "dataset_path = '.'\n",
        "# Define batch size for the DataLoader\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = STL10(\n",
        "    root=dataset_path,\n",
        "    download=True,\n",
        "    split=\"train\",\n",
        "    transform=preprocess\n",
        ")\n",
        "\n",
        "# Create a DataLoader for training dataset\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_dataset = STL10(\n",
        "    root=dataset_path,\n",
        "    download=True,\n",
        "    split=\"test\"\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero Shot"
      ],
      "metadata": {
        "id": "9ydwIVw4lK4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "\n",
        "for image, label in tqdm.tqdm(test_dataset):\n",
        "    input_tensor = preprocess(image).unsqueeze(0)\n",
        "    vision_embeddings = model.encode_image(input_tensor)\n",
        "    output_class_probs = embeddings_to_class_probs(vision_embeddings, text_embeddings)\n",
        "    output_label = torch.argmax(output_class_probs, dim=-1)\n",
        "    num_correct += int(torch.count_nonzero(output_label == label))\n",
        "\n",
        "accuracy = 100. * num_correct / len(test_dataset)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "fPZmTojek5pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syYLuMsvh-Ji"
      },
      "source": [
        "### Linear head for *classification*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval the model\n",
        "\n",
        "num_correct = 0\n",
        "\n",
        "for image, label in tqdm.tqdm(test_dataset):\n",
        "    input_tensor = preprocess(image).unsqueeze(0)\n",
        "    vision_embeddings = model.encode_image(input_tensor)\n",
        "    output_logits = linear_probe(vision_embeddings)\n",
        "    output_logprob = F.log_softmax(output_logits, dim=-1)\n",
        "    output_label = torch.argmax(output_logprob, dim=-1)\n",
        "    num_correct += int(torch.count_nonzero(output_label == label))\n",
        "\n",
        "accuracy = 100. * num_correct / len(dataset)\n",
        "print(accuracy) ## should be 98 (bound to be verified)"
      ],
      "metadata": {
        "id": "MCTuAe3bkZin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR10 Dataset"
      ],
      "metadata": {
        "id": "6gvmJMEZRryU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\n",
        "    \"an airplane\",\n",
        "    \"an automobile\",\n",
        "    \"a bird\",\n",
        "    \"a cat\",\n",
        "    \"a deer\",\n",
        "    \"a dog\",\n",
        "    \"a frog\",\n",
        "    \"a horse\",\n",
        "    \"a ship\",\n",
        "    \"a truck\"\n",
        "]\n",
        "\n",
        "text = tokenizer(labels).to(device)\n",
        "text_embeddings = model.encode_text(text).to(device)\n",
        "\n",
        "linear_probe = nn.Linear(512, len(labels)).to(device)"
      ],
      "metadata": {
        "id": "kFU9Zr94GTr4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset_path = '.'\n",
        "\n",
        "optimizer = torch.optim.Adam(linear_probe.parameters(), lr=3e-4)\n",
        "\n",
        "train_dataset = CIFAR10(\n",
        "    root=dataset_path,\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=preprocess\n",
        ")\n",
        "\n",
        "test_dataset = CIFAR10(\n",
        "    root=dataset_path,\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=None\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create a DataLoader for training dataset\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "H1ZReFiXGQ1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0d39d9-2d0f-439c-9ce7-9f0e7ec0dce6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Zero Shot"
      ],
      "metadata": {
        "id": "UFyNz4YDBjf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "\n",
        "for image, label in tqdm.tqdm(test_dataset):\n",
        "    input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "    vision_embeddings = model.encode_image(input_tensor).to(device)\n",
        "    output_class_probs = embeddings_to_class_probs(vision_embeddings, text_embeddings)\n",
        "    output_label = torch.argmax(output_class_probs, dim=-1)\n",
        "    num_correct += int(torch.count_nonzero(output_label == label))\n",
        "\n",
        "accuracy = 100. * num_correct / len(test_dataset)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "YmnHGsMiBmzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear head for classification\n"
      ],
      "metadata": {
        "id": "Gow56kQIBsAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "model.eval() # freeze the clip model, we are only training the linear layer\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    for input_tensor, label in iter(tqdm.tqdm(train_loader)):\n",
        "        input_tensor , label = input_tensor.to(device), label.to(device)\n",
        "        vision_embeddings = model.encode_image(input_tensor)\n",
        "        optimizer.zero_grad()\n",
        "        output_logits = linear_probe(vision_embeddings)\n",
        "        output_logprob = F.log_softmax(output_logits, dim=-1)\n",
        "        loss = F.nll_loss(output_logprob, label)\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_average_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    losses.append(epoch_average_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "jlLMdaX6R9Af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31525770-62ff-4875-aa72-9883d1c35811"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:51<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.4414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:50<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 2.3688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:50<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 2.2351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:50<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 2.3377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:50<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 2.3204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:51<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 2.2168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:51<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 2.3108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:50<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 2.2826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:48<00:00,  1.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 2.3488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [06:50<00:00,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 2.2735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "\n",
        "for image, label in tqdm.tqdm(test_dataset):\n",
        "    input_tensor = preprocess(image).unsqueeze(0)\n",
        "    vision_embeddings = model.encode_image(input_tensor)\n",
        "    output_class_probs = embeddings_to_class_probs(vision_embeddings, text_embeddings)\n",
        "    output_label = torch.argmax(output_class_probs, dim=-1)\n",
        "    num_correct += int(torch.count_nonzero(output_label == label))\n",
        "\n",
        "accuracy = 100. * num_correct / len(test_dataset)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "JtJ9qVDkSNFa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "bf9f8084-f079-447a-db26-715e6ae8fe8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tqdm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-69e002e7d794>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvision_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOitlSmxsDlH"
      },
      "source": [
        "### Person ReId"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSUnOuYXs0-H"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFile\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import os.path as osp\n",
        "import random\n",
        "import torch\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "\n",
        "def read_image(img_path):\n",
        "    \"\"\"Keep reading image until succeed.\n",
        "    This can avoid IOError incurred by heavy IO process.\"\"\"\n",
        "    got_img = False\n",
        "    if not osp.exists(img_path):\n",
        "        raise IOError(\"{} does not exist\".format(img_path))\n",
        "    while not got_img:\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            got_img = True\n",
        "        except IOError:\n",
        "            print(\"IOError incurred when reading '{}'. Will redo. Don't worry. Just chill.\".format(img_path))\n",
        "            pass\n",
        "    return img\n",
        "\n",
        "\n",
        "class BaseDataset(object):\n",
        "    \"\"\"\n",
        "    Base class of reid dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def get_imagedata_info(self, data):\n",
        "        pids, cams, tracks = [], [], []\n",
        "        for _, pid, camid, trackid in data:\n",
        "            pids += [pid]\n",
        "            cams += [camid]\n",
        "            tracks += [trackid]\n",
        "        pids = set(pids)\n",
        "        cams = set(cams)\n",
        "        tracks = set(tracks)\n",
        "        num_pids = len(pids)\n",
        "        num_cams = len(cams)\n",
        "        num_imgs = len(data)\n",
        "        num_views = len(tracks)\n",
        "        return num_pids, num_imgs, num_cams, num_views\n",
        "\n",
        "    def print_dataset_statistics(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class BaseImageDataset(BaseDataset):\n",
        "    \"\"\"\n",
        "    Base class of image reid dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def print_dataset_statistics(self, train, query, gallery):\n",
        "        num_train_pids, num_train_imgs, num_train_cams, num_train_views = self.get_imagedata_info(train)\n",
        "        num_query_pids, num_query_imgs, num_query_cams, num_train_views = self.get_imagedata_info(query)\n",
        "        num_gallery_pids, num_gallery_imgs, num_gallery_cams, num_train_views = self.get_imagedata_info(gallery)\n",
        "\n",
        "        print(\"Dataset statistics:\")\n",
        "        print(\"  ----------------------------------------\")\n",
        "        print(\"  subset   | # ids | # images | # cameras\")\n",
        "        print(\"  ----------------------------------------\")\n",
        "        print(\"  train    | {:5d} | {:8d} | {:9d}\".format(num_train_pids, num_train_imgs, num_train_cams))\n",
        "        print(\"  query    | {:5d} | {:8d} | {:9d}\".format(num_query_pids, num_query_imgs, num_query_cams))\n",
        "        print(\"  gallery  | {:5d} | {:8d} | {:9d}\".format(num_gallery_pids, num_gallery_imgs, num_gallery_cams))\n",
        "        print(\"  ----------------------------------------\")\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path, pid, camid, trackid = self.dataset[index]\n",
        "        img = read_image(img_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, pid, camid, trackid, img_path.split('/')[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1JeZOVysvyT"
      },
      "outputs": [],
      "source": [
        "# encoding: utf-8\n",
        "\"\"\"\n",
        "@author:  sherlock\n",
        "@contact: sherlockliao01@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "import glob\n",
        "import re\n",
        "\n",
        "import os.path as osp\n",
        "\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "class Market1501(BaseImageDataset):\n",
        "    \"\"\"\n",
        "    Market1501\n",
        "    Reference:\n",
        "    Zheng et al. Scalable Person Re-identification: A Benchmark. ICCV 2015.\n",
        "    URL: http://www.liangzheng.org/Project/project_reid.html\n",
        "\n",
        "    Dataset statistics:\n",
        "    # identities: 1501 (+1 for background)\n",
        "    # images: 12936 (train) + 3368 (query) + 15913 (gallery)\n",
        "    \"\"\"\n",
        "    dataset_dir = 'Market-1501-v15.09.15'\n",
        "\n",
        "    def __init__(self, root='', verbose=True, pid_begin = 0, **kwargs):\n",
        "        super(Market1501, self).__init__()\n",
        "        self.dataset_dir = osp.join(root, self.dataset_dir)\n",
        "        self.train_dir = osp.join(self.dataset_dir, 'bounding_box_train')\n",
        "        self.query_dir = osp.join(self.dataset_dir, 'query')\n",
        "        self.gallery_dir = osp.join(self.dataset_dir, 'bounding_box_test')\n",
        "\n",
        "        self._check_before_run()\n",
        "        self.pid_begin = pid_begin\n",
        "        train = self._process_dir(self.train_dir, relabel=True)\n",
        "        query = self._process_dir(self.query_dir, relabel=False)\n",
        "        gallery = self._process_dir(self.gallery_dir, relabel=False)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"=> Market1501 loaded\")\n",
        "            self.print_dataset_statistics(train, query, gallery)\n",
        "\n",
        "        self.train = train\n",
        "        self.query = query\n",
        "        self.gallery = gallery\n",
        "\n",
        "        self.num_train_pids, self.num_train_imgs, self.num_train_cams, self.num_train_vids = self.get_imagedata_info(self.train)\n",
        "        self.num_query_pids, self.num_query_imgs, self.num_query_cams, self.num_query_vids = self.get_imagedata_info(self.query)\n",
        "        self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams, self.num_gallery_vids = self.get_imagedata_info(self.gallery)\n",
        "\n",
        "    def _check_before_run(self):\n",
        "        \"\"\"Check if all files are available before going deeper\"\"\"\n",
        "        if not osp.exists(self.dataset_dir):\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.dataset_dir))\n",
        "        if not osp.exists(self.train_dir):\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\n",
        "        if not osp.exists(self.query_dir):\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.query_dir))\n",
        "        if not osp.exists(self.gallery_dir):\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.gallery_dir))\n",
        "\n",
        "    def _process_dir(self, dir_path, relabel=False):\n",
        "        img_paths = glob.glob(osp.join(dir_path, '*.jpg'))\n",
        "        pattern = re.compile(r'([-\\d]+)_c(\\d)')\n",
        "\n",
        "        pid_container = set()\n",
        "        for img_path in sorted(img_paths):\n",
        "            pid, _ = map(int, pattern.search(img_path).groups())\n",
        "            if pid == -1: continue  # junk images are just ignored\n",
        "            pid_container.add(pid)\n",
        "        pid2label = {pid: label for label, pid in enumerate(pid_container)}\n",
        "        dataset = []\n",
        "        for img_path in sorted(img_paths):\n",
        "            pid, camid = map(int, pattern.search(img_path).groups())\n",
        "            if pid == -1: continue  # junk images are just ignored\n",
        "            assert 0 <= pid <= 1501  # pid == 0 means background\n",
        "            assert 1 <= camid <= 6\n",
        "            camid -= 1  # index starts from 0\n",
        "            if relabel: pid = pid2label[pid]\n",
        "\n",
        "            dataset.append((img_path, self.pid_begin + pid, camid, 0))\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwM46YYiswjJ"
      },
      "outputs": [],
      "source": [
        "!pip install yacs\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from yacs.config import CfgNode as CN\n",
        "\n",
        "# cfg = CN()\n",
        "\n",
        "# cfg.merge_from_file('./vit_clipreid.yml')\n",
        "# cfg.freeze()\n",
        "\n",
        "val_transforms = T.Compose([\n",
        "    # T.Resize(cfg.INPUT.SIZE_TEST),\n",
        "    T.Resize([256, 128]),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "dataset = Market1501(root=(''))\n",
        "\n",
        "market1501_val_set = ImageDataset(dataset.query + dataset.gallery, val_transforms)\n",
        "\n",
        "market1501_val_loader = DataLoader(\n",
        "    market1501_val_set, batch_size=32, shuffle=False,\n",
        "    # collate_fn=val_collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cckZGr2uaPS"
      },
      "outputs": [],
      "source": [
        "train_transforms = T.Compose([\n",
        "            T.Resize([256, 128], interpolation=3),\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.Pad(10),\n",
        "            T.RandomCrop([256, 128]),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "            # RandomErasing(probability=cfg.INPUT.RE_PROB, mode='pixel', max_count=1, device='cpu'),\n",
        "            # RandomErasing(probability=cfg.INPUT.RE_PROB, mean=cfg.INPUT.PIXEL_MEAN)\n",
        "        ])\n",
        "\n",
        "train_set = ImageDataset(dataset.train, train_transforms)\n",
        "train_set_normal = ImageDataset(dataset.train, val_transforms)\n",
        "\n",
        "market1501_train_loader = DataLoader(\n",
        "    train_set, batch_size=32,\n",
        "    sampler=RandomIdentitySampler(dataset.train, 64, 4),\n",
        "    num_workers = 8,\n",
        "    # collate_fn=train_collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Linear head for classification\n",
        "\n"
      ],
      "metadata": {
        "id": "pXu5_zTEZ4aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "linear_probe = nn.Linear(512, 1501)"
      ],
      "metadata": {
        "id": "GsclXuXBbBXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z-u5o5tul7T"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import STL10\n",
        "\n",
        "optimizer = torch.optim.Adam(linear_probe.parameters(), lr=3e-4)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for input_tensor, label in iter(tqdm.tqdm(market1501_train_loader)):\n",
        "        vision_embeddings = model.encode_image(input_tensor)\n",
        "        optimizer.zero_grad()\n",
        "        output_logits = linear_probe(vision_embeddings)\n",
        "        output_logprob = F.log_softmax(output_logits, dim=-1)\n",
        "        loss = F.nll_loss(output_logprob, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "num_correct = 0\n",
        "\n",
        "for image, label in tqdm.tqdm(market1501_val_loader):\n",
        "    input_tensor = preprocess(image).unsqueeze(0)\n",
        "    vision_embeddings = model.encode_image(input_tensor)\n",
        "    output_class_probs = embeddings_to_class_probs(vision_embeddings, text_embeddings)\n",
        "    output_label = torch.argmax(output_class_probs, dim=-1)\n",
        "    num_correct += int(torch.count_nonzero(output_label == label))\n",
        "\n",
        "accuracy = 100. * num_correct / len(dataset)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "8dEnfUeVafrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Distillation\n"
      ],
      "metadata": {
        "id": "Re63OcIroSKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the ResNet18 model as the student model\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=True)\n",
        "        # Modify the classifier layer for your specific task\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet18(x)\n",
        "\n",
        "# Knowledge Distillation Loss\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "\n",
        "    def forward(self, outputs_student, outputs_teacher):\n",
        "        return nn.MSELoss()(outputs_student, outputs_teacher)\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ProjectionHead, self).__init__()\n",
        "        self.projection_head = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.projection_head(x)"
      ],
      "metadata": {
        "id": "sMBnAzNzjo2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Contrastive* Relational Distillation"
      ],
      "metadata": {
        "id": "F_Q2Mwa6jqKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Knowledge Distillation\n",
        "student_model = ResNet18()\n",
        "projection_head = ProjectionHead(1000, 512)  # Projection head outside ViT\n",
        "distillation_loss = DistillationLoss()\n",
        "\n",
        "# Define optimizer and learning rate\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "\n",
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3  # Adjust as needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "student_model.to(device)\n",
        "projection_head.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []  # Store losses for each epoch\n",
        "    for inputs, labels in tqdm.tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass on the teacher model\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = model.encode_image(inputs)\n",
        "\n",
        "        # Forward pass on the student model\n",
        "        student_outputs = student_model(inputs)\n",
        "        student_proj = projection_head(student_outputs)\n",
        "\n",
        "        # Compute the distillation loss\n",
        "        loss = distillation_loss(student_proj, teacher_outputs)\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_average_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    losses.append(epoch_average_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "ax7pcijddO2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Zero shot evaluation of ResNet18\n",
        "\n",
        "num_correct = 0\n",
        "\n",
        "for image, label in tqdm.tqdm(dataset):\n",
        "    input_tensor = preprocess(image).unsqueeze(0)\n",
        "    student_outputs = student_model(input_tensor)\n",
        "    vistion_embeddings = projection_head(student_outputs)\n",
        "    output_class_probs = embeddings_to_class_probs(vision_embeddings, text_embeddings)\n",
        "    output_label = torch.argmax(output_class_probs, dim=-1)\n",
        "    num_correct += int(torch.count_nonzero(output_label == label))\n",
        "\n",
        "accuracy = 100. * num_correct / len(dataset)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "ViZvqET9LdYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Distillation"
      ],
      "metadata": {
        "id": "2qfYoDvaoYuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Knowledge Distillation\n",
        "student_model = ResNet18()\n",
        "student_model.load_state_dict(torch.load('resnet18_student_model.pth'))\n",
        "projection_head = ProjectionHead(1000, 512)  # Projection head outside ViT\n",
        "distillation_loss = DistillationLoss()\n",
        "\n",
        "# Define optimizer and learning rate\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "\n",
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20  # Adjust as needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "student_model.to(device)\n",
        "projection_head.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []  # Store losses for each epoch\n",
        "    for inputs, labels in tqdm.tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass on the teacher model\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = model.encode_image(inputs)\n",
        "\n",
        "        # Forward pass on the student model\n",
        "        student_outputs = student_model(inputs)\n",
        "        student_proj = projection_head(student_outputs)\n",
        "\n",
        "        # Compute the distillation loss\n",
        "        loss = distillation_loss(student_proj, teacher_outputs)\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_average_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    losses.append(epoch_average_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LfqRO6uoeiA",
        "outputId": "b7cfb15d-84e5-4abf-a4f8-30ee401f7416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 79/79 [00:31<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.1050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:30<00:00,  2.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Loss: 0.0880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:30<00:00,  2.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Loss: 0.1086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:30<00:00,  2.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Loss: 0.0880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:30<00:00,  2.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 0.0799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:30<00:00,  2.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Loss: 0.0681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:30<00:00,  2.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Loss: 0.0726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:30<00:00,  2.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Loss: 0.0698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 35/79 [00:13<00:17,  2.59it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curve\n",
        "plt.plot(losses, label='Distillation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Knowledge Distillation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the trained student model\n",
        "torch.save(student_model.state_dict(), 'resnet18_student_model.pth')"
      ],
      "metadata": {
        "id": "4mq2qwJEBNtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Zero shot evaluation of ResNet18\n",
        "num_correct = 0\n",
        "\n",
        "for image, label in tqdm.tqdm(test_dataset):\n",
        "    input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "    student_outputs = student_model(input_tensor)\n",
        "    vision_embeddings = projection_head(student_outputs).to(device)\n",
        "    output_class_probs = embeddings_to_class_probs(vision_embeddings, text_embeddings)\n",
        "    output_label = torch.argmax(output_class_probs, dim=-1)\n",
        "    num_correct += int(torch.count_nonzero(output_label == label))\n",
        "\n",
        "accuracy = 100. * num_correct / len(test_dataset)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "r8rY-d1_EWhe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ccc1472ecdf42d69e0917e2f631f76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b136b9003bef44838e369753e1a50252",
              "IPY_MODEL_d2527b3edcf2410b88af18d2529062a4",
              "IPY_MODEL_7540edfe79074ab68decbadd125bb7b4"
            ],
            "layout": "IPY_MODEL_15152bbcd18d40989aac264aca6d5b1c"
          }
        },
        "b136b9003bef44838e369753e1a50252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b5a6b7e557c445e8ce85b0888caa68e",
            "placeholder": "​",
            "style": "IPY_MODEL_7d876db4f40148f299df41e888691e14",
            "value": "open_clip_pytorch_model.bin: 100%"
          }
        },
        "d2527b3edcf2410b88af18d2529062a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f80326cb8b3e4459a69d01ade309db93",
            "max": 605219813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ff2c7f3aeae42a0b285826c7c65569f",
            "value": 605219813
          }
        },
        "7540edfe79074ab68decbadd125bb7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea5ce38ca2884ff1beee617a37f37b9e",
            "placeholder": "​",
            "style": "IPY_MODEL_5030d2373dfd492aaedf703d8ecac028",
            "value": " 605M/605M [00:02&lt;00:00, 309MB/s]"
          }
        },
        "15152bbcd18d40989aac264aca6d5b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b5a6b7e557c445e8ce85b0888caa68e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d876db4f40148f299df41e888691e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f80326cb8b3e4459a69d01ade309db93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ff2c7f3aeae42a0b285826c7c65569f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea5ce38ca2884ff1beee617a37f37b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5030d2373dfd492aaedf703d8ecac028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}